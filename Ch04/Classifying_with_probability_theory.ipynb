{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Classifying text with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Prepare: making word vectors from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 6\n",
      "Number of labels: 6\n",
      "['mr', 'garbage', 'stop', 'dalmation', 'park', 'how', 'dog', 'love', 'quit', 'to', 'steak', 'food', 'maybe', 'take', 'worthless', 'stupid', 'ate', 'him', 'please', 'so', 'buying', 'cute', 'has', 'is', 'help', 'flea', 'I', 'not', 'problems', 'my', 'posting', 'licks']\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                    ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                    ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                    ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                    ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not abusive\n",
    "    return postingList, classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) # create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # union of the two sets\n",
    "    return list(vocabSet) # convert to list and return\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) # create a list of 0's\n",
    "    for word in inputSet:\n",
    "        if word in vocabList: # check if the word is in the vocab list\n",
    "            returnVec[vocabList.index(word)] = 1 # set the index to 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec # return the vector representation of the input set\n",
    "\n",
    "listOPosts, listClasses = loadDataSet() # load the dataset\n",
    "print(\"Number of documents:\", len(listOPosts))\n",
    "print(\"Number of labels:\", len(listClasses))\n",
    "myVocabList = createVocabList(listOPosts) # create the vocab list\n",
    "print(myVocabList) # print the vocab list\n",
    "print(setOfWords2Vec(myVocabList, listOPosts[0])) # print the vector representation of the first document\n",
    "print(setOfWords2Vec(myVocabList, listOPosts[3])) # print the vector representation of the fourth document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Train: calculating probabilities from word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654\n",
      " -3.25809654 -3.25809654 -3.25809654 -3.25809654 -2.56494936 -2.15948425\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -1.87180218\n",
      " -3.25809654 -2.56494936]\n",
      "[-3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -2.35137526 -2.35137526 -3.04452244 -2.35137526\n",
      " -2.35137526 -2.35137526 -1.94591015 -1.65822808 -3.04452244 -2.35137526\n",
      " -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -2.35137526 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix) # number of training documents\n",
    "    numWords = len(trainMatrix[0]) # number of words in the vocab list\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs) # probability of abusive class\n",
    "    p0Num = np.ones(numWords) # initialize p0Num to a zero vector of size numWords\n",
    "    p1Num = np.ones(numWords) # initialize p1Num to a zero vector of size numWords\n",
    "    p0Denom = 2.0 # initialize p0Denom to 2.0 (to avoid zero division)\n",
    "    p1Denom = 2.0 # initialize p1Denom to 2.0 (to avoid zero division)\n",
    "    for i in range(numTrainDocs): # iterate through all training documents\n",
    "        if trainCategory[i] == 1: # if the document is abusive\n",
    "            p1Num += trainMatrix[i] # add the word vector to p1Num\n",
    "            p1Denom += sum(trainMatrix[i]) # add the sum of the word vector to p1Denom\n",
    "        else: # if the document is not abusive\n",
    "            p0Num += trainMatrix[i] # add the word vector to p0Num\n",
    "            p0Denom += sum(trainMatrix[i]) # add the sum of the word vector to p0Denom\n",
    "    p1Vect = np.log(p1Num / p1Denom) # calculate log probability of abusive class\n",
    "    p0Vect = np.log(p0Num / p0Denom) # calculate log probability of non-abusive class\n",
    "    return p0Vect, p1Vect, pAbusive # return the probabilities\n",
    "\n",
    "trainMat = [] # initialize the training matrix\n",
    "for postinDoc in listOPosts: # iterate through all documents\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) # append the vector representation of the document to the training matrix\n",
    "    \n",
    "p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses)) # train the Naive Bayes classifier\n",
    "print(p0V) # print the log probability of non-abusive class\n",
    "print(p1V) # print the log probability of abusive class\n",
    "print(pAb) # print the probability of abusive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Test: modifying the classifier for real-world conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) # calculate the log probability of abusive class\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) # calculate the log probability of non-abusive class\n",
    "    if p1 > p0: return 1 # if the log probability of abusive class is greater, return 1\n",
    "    else: return 0 # otherwise, return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet() # load the dataset\n",
    "    myVocabList = createVocabList(listOPosts) # create the vocab list\n",
    "    trainMat = [] # initialize the training matrix\n",
    "    for postinDoc in listOPosts: # iterate through all documents\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) # append the vector representation of the document to the training matrix\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses)) # train the Naive Bayes classifier\n",
    "    testEntry = ['love', 'my', 'dalmation'] # test entry 1\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) # convert to vector representation\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb)) # classify and print result\n",
    "    testEntry = ['stupid', 'garbage'] # test entry 2\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) # convert to vector representation\n",
    "    print(testEntry, 'classified as:', classifyNB(thisDoc, p0V, p1V, pAb)) # classify and print result\n",
    "    \n",
    "testingNB() # call the testing function to test the classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Prepare: the bag-of-words document model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList) # create a list of 0's\n",
    "    for word in inputSet: # iterate through the input set\n",
    "        if word in vocabList: # check if the word is in the vocab list\n",
    "            returnVec[vocabList.index(word)] += 1 # increment the index by 1\n",
    "    return returnVec # return the vector representation of the input set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Example: classifying spam email with naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 Prepare: tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 Test: cross validation with naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def textParse(bigString): # function to parse the text\n",
    "    listOfTokens = re.split(r'\\W*', bigString) # split the string into tokens\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] # convert to lowercase and filter out short tokens\n",
    "\n",
    "def spamTest():\n",
    "    docList = [] # initialize the document list\n",
    "    classList = [] # initialize the class list\n",
    "    fullText = [] # initialize the full text list\n",
    "    for i in range(1, 26): # iterate through the spam and ham directories\n",
    "        wordList = textParse(open('email/spam/%d.txt' %i, encoding='utf-8', errors='ignore').read()) # read and parse the spam emails\n",
    "        docList.append(wordList) # append to the document list\n",
    "        fullText.extend(wordList) # extend the full text list with the word list\n",
    "        classList.append(1) # append 1 for spam class\n",
    "        wordList = textParse(open('email/ham/%d.txt' %i , encoding='utf-8', errors='ignore').read()) # read and parse the ham emails\n",
    "        docList.append(wordList) # append to the document list\n",
    "        fullText.extend(wordList) # extend the full text list with the word list\n",
    "        classList.append(0) # append 0 for ham class\n",
    "        \n",
    "    vocabList = createVocabList(docList) # create the vocab list from the document list\n",
    "    trainingSet = list(range(50)) # create a training set of size 50\n",
    "    testSet = [] # initialize the test set\n",
    "    for i in range(10): # iterate through 10 times to create test set\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet))) # generate a random index\n",
    "        testSet.append(trainingSet[randIndex]) # append to the test set\n",
    "        del(trainingSet[randIndex]) # delete from the training set\n",
    "        \n",
    "    trainMat = [] # initialize the training matrix\n",
    "    trainClasses = [] # initialize the training classes\n",
    "    for docIndex in trainingSet: # iterate through all documents in training set\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex])) # append vector representation to training matrix\n",
    "        trainClasses.append(classList[docIndex]) # append class label to training classes\n",
    "        \n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses)) # train the Naive Bayes classifier\n",
    "    \n",
    "    errorCount = 0.0 # initialize error count to 0.0\n",
    "    \n",
    "    for docIndex in testSet: # iterate through all documents in test set\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex]) # convert to  vector representation\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1.0\n",
    "            \n",
    "    print(\"the error rate is:\", float(errorCount) / len(testSet)) # print the error rate\n",
    "    \n",
    "spamTest() # call the spam test function to test the classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
